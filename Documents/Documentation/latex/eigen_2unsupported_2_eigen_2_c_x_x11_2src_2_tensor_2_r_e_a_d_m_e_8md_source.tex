\hypertarget{eigen_2unsupported_2_eigen_2_c_x_x11_2src_2_tensor_2_r_e_a_d_m_e_8md_source}{}\section{R\+E\+A\+D\+M\+E.\+md}

\begin{DoxyCode}
00001 # Eigen Tensors
00002 
00003 Tensors are multidimensional arrays of elements. Elements are typically scalars,
00004 but more complex types such as strings are also supported.
00005 
00006 [TOC]
00007 
00008 ## Tensor Classes
00009 
00010 You can manipulate a tensor with one of the following classes.  They all are in
00011 the namespace ```::Eigen.```
00012 
00013 
00014 ### Class Tensor<data\_type, rank>
00015 
00016 This is the class to use to create a tensor and allocate memory for it.  The
00017 class is templatized with the tensor datatype, such as float or int, and the
00018 tensor rank.  The rank is the number of dimensions, for example rank 2 is a
00019 matrix.
00020 
00021 Tensors of this class are resizable.  For example, if you assign a tensor of a
00022 different size to a Tensor, that tensor is resized to match its new value.
00023 
00024 #### Constructor Tensor<data\_type, rank>(size0, size1, ...)
00025 
00026 Constructor for a Tensor.  The constructor must be passed ```rank``` integers
00027 indicating the sizes of the instance along each of the the ```rank```
00028 dimensions.
00029 
00030     // Create a tensor of rank 3 of sizes 2, 3, 4.  This tensor owns
00031     // memory to hold 24 floating point values (24 = 2 x 3 x 4).
00032     Tensor<float, 3> t\_3d(2, 3, 4);
00033 
00034     // Resize t\_3d by assigning a tensor of different sizes, but same rank.
00035     t\_3d = Tensor<float, 3>(3, 4, 3);
00036 
00037 #### Constructor Tensor<data\_type, rank>(size\_array)
00038 
00039 Constructor where the sizes for the constructor are specified as an array of
00040 values instead of an explicitly list of parameters.  The array type to use is
00041 ```Eigen::array<Eigen::Index>```.  The array can be constructed automatically
00042 from an initializer list.
00043 
00044     // Create a tensor of strings of rank 2 with sizes 5, 7.
00045     Tensor<string, 2> t\_2d(\{5, 7\});
00046 
00047 
00048 ### Class TensorFixedSize<data\_type, Sizes<size0, size1, ...>>
00049 
00050 Class to use for tensors of fixed size, where the size is known at compile
00051 time.  Fixed sized tensors can provide very fast computations because all their
00052 dimensions are known by the compiler.  FixedSize tensors are not resizable.
00053 
00054 If the total number of elements in a fixed size tensor is small enough the
00055 tensor data is held onto the stack and does not cause heap allocation and free.
00056 
00057     // Create a 4 x 3 tensor of floats.
00058     TensorFixedSize<float, Sizes<4, 3>> t\_4x3;
00059 
00060 ### Class TensorMap<Tensor<data\_type, rank>>
00061 
00062 This is the class to use to create a tensor on top of memory allocated and
00063 owned by another part of your code.  It allows to view any piece of allocated
00064 memory as a Tensor.  Instances of this class do not own the memory where the
00065 data are stored.
00066 
00067 A TensorMap is not resizable because it does not own the memory where its data
00068 are stored.
00069 
00070 #### Constructor TensorMap<Tensor<data\_type, rank>>(data, size0, size1, ...)
00071 
00072 Constructor for a Tensor.  The constructor must be passed a pointer to the
00073 storage for the data, and "rank" size attributes.  The storage has to be
00074 large enough to hold all the data.
00075 
00076     // Map a tensor of ints on top of stack-allocated storage.
00077     int storage[128];  // 2 x 4 x 2 x 8 = 128
00078     TensorMap<Tensor<int, 4>> t\_4d(storage, 2, 4, 2, 8);
00079 
00080     // The same storage can be viewed as a different tensor.
00081     // You can also pass the sizes as an array.
00082     TensorMap<Tensor<int, 2>> t\_2d(storage, 16, 8);
00083 
00084     // You can also map fixed-size tensors.  Here we get a 1d view of
00085     // the 2d fixed-size tensor.
00086     Tensor<float, Sizes<4, 5>> t\_4x3;
00087     TensorMap<Tensor<float, 1>> t\_12(t\_4x3, 12);
00088 
00089 
00090 #### Class TensorRef
00091 
00092 See Assigning to a TensorRef below.
00093 
00094 ## Accessing Tensor Elements
00095 
00096 #### <data\_type> tensor(index0, index1...)
00097 
00098 Return the element at position ```(index0, index1...)``` in tensor
00099 ```tensor```.  You must pass as many parameters as the rank of ```tensor```.
00100 The expression can be used as an l-value to set the value of the element at the
00101 specified position.  The value returned is of the datatype of the tensor.
00102 
00103     // Set the value of the element at position (0, 1, 0);
00104     Tensor<float, 3> t\_3d(2, 3, 4);
00105     t\_3d(0, 1, 0) = 12.0f;
00106 
00107     // Initialize all elements to random values.
00108     for (int i = 0; i < 2; ++i) \{
00109       for (int j = 0; j < 3; ++j) \{
00110         for (int k = 0; k < 4; ++k) \{
00111           t\_3d(i, j, k) = ...some random value...;
00112         \}
00113       \}
00114     \}
00115 
00116     // Print elements of a tensor.
00117     for (int i = 0; i < 2; ++i) \{
00118       LOG(INFO) << t\_3d(i, 0, 0);
00119     \}
00120 
00121 
00122 ## TensorLayout
00123 
00124 The tensor library supports 2 layouts: ```ColMajor``` (the default) and
00125 ```RowMajor```.  Only the default column major layout is currently fully
00126 supported, and it is therefore not recommended to attempt to use the row major
00127 layout at the moment.
00128 
00129 The layout of a tensor is optionally specified as part of its type. If not
00130 specified explicitly column major is assumed.
00131 
00132     Tensor<float, 3, ColMajor> col\_major;  // equivalent to Tensor<float, 3>
00133     TensorMap<Tensor<float, 3, RowMajor> > row\_major(data, ...);
00134 
00135 All the arguments to an expression must use the same layout. Attempting to mix
00136 different layouts will result in a compilation error.
00137 
00138 It is possible to change the layout of a tensor or an expression using the
00139 ```swap\_layout()``` method.  Note that this will also reverse the order of the
00140 dimensions.
00141 
00142     Tensor<float, 2, ColMajor> col\_major(2, 4);
00143     Tensor<float, 2, RowMajor> row\_major(2, 4);
00144 
00145     Tensor<float, 2> col\_major\_result = col\_major;  // ok, layouts match
00146     Tensor<float, 2> col\_major\_result = row\_major;  // will not compile
00147 
00148     // Simple layout swap
00149     col\_major\_result = row\_major.swap\_layout();
00150     eigen\_assert(col\_major\_result.dimension(0) == 4);
00151     eigen\_assert(col\_major\_result.dimension(1) == 2);
00152 
00153     // Swap the layout and preserve the order of the dimensions
00154     array<int, 2> shuffle(1, 0);
00155     col\_major\_result = row\_major.swap\_layout().shuffle(shuffle);
00156     eigen\_assert(col\_major\_result.dimension(0) == 2);
00157     eigen\_assert(col\_major\_result.dimension(1) == 4);
00158 
00159 
00160 ## Tensor Operations
00161 
00162 The Eigen Tensor library provides a vast library of operations on Tensors:
00163 numerical operations such as addition and multiplication, geometry operations
00164 such as slicing and shuffling, etc.  These operations are available as methods
00165 of the Tensor classes, and in some cases as operator overloads.  For example
00166 the following code computes the elementwise addition of two tensors:
00167 
00168     Tensor<float, 3> t1(2, 3, 4);
00169     ...set some values in t1...
00170     Tensor<float, 3> t2(2, 3, 4);
00171     ...set some values in t2...
00172     // Set t3 to the element wise sum of t1 and t2
00173     Tensor<float, 3> t3 = t1 + t2;
00174 
00175 While the code above looks easy enough, it is important to understand that the
00176 expression ```t1 + t2``` is not actually adding the values of the tensors.  The
00177 expression instead constructs a "tensor operator" object of the class
00178 TensorCwiseBinaryOp<scalar\_sum>, which has references to the tensors
00179 ```t1``` and ```t2```.  This is a small C++ object that knows how to add
00180 ```t1``` and ```t2```.  It is only when the value of the expression is assigned
00181 to the tensor ```t3``` that the addition is actually performed.  Technically,
00182 this happens through the overloading of ```operator=()``` in the Tensor class.
00183 
00184 This mechanism for computing tensor expressions allows for lazy evaluation and
00185 optimizations which are what make the tensor library very fast.
00186 
00187 Of course, the tensor operators do nest, and the expression ```t1 + t2 *
00188 0.3f``` is actually represented with the (approximate) tree of operators:
00189 
00190     TensorCwiseBinaryOp<scalar\_sum>(t1, TensorCwiseUnaryOp<scalar\_mul>(t2, 0.3f))
00191 
00192 
00193 ### Tensor Operations and C++ "auto"
00194 
00195 Because Tensor operations create tensor operators, the C++ ```auto``` keyword
00196 does not have its intuitive meaning.  Consider these 2 lines of code:
00197 
00198     Tensor<float, 3> t3 = t1 + t2;
00199     auto t4 = t1 + t2;
00200 
00201 In the first line we allocate the tensor ```t3``` and it will contain the
00202 result of the addition of ```t1``` and ```t2```.  In the second line, ```t4```
00203 is actually the tree of tensor operators that will compute the addition of
00204 ```t1``` and ```t2```.  In fact, ```t4``` is *not* a tensor and you cannot get
00205 the values of its elements:
00206 
00207     Tensor<float, 3> t3 = t1 + t2;
00208     cout << t3(0, 0, 0);  // OK prints the value of t1(0, 0, 0) + t2(0, 0, 0)
00209 
00210     auto t4 = t1 + t2;
00211     cout << t4(0, 0, 0);  // Compilation error!
00212 
00213 When you use ```auto``` you do not get a Tensor as a result but instead a
00214 non-evaluated expression.  So only use ```auto``` to delay evaluation.
00215 
00216 Unfortunately, there is no single underlying concrete type for holding
00217 non-evaluated expressions, hence you have to use auto in the case when you do
00218 want to hold non-evaluated expressions.
00219 
00220 When you need the results of set of tensor computations you have to assign the
00221 result to a Tensor that will be capable of holding onto them.  This can be
00222 either a normal Tensor, a fixed size Tensor, or a TensorMap on an existing
00223 piece of memory.  All the following will work:
00224 
00225     auto t4 = t1 + t2;
00226 
00227     Tensor<float, 3> result = t4;  // Could also be: result(t4);
00228     cout << result(0, 0, 0);
00229 
00230     TensorMap<float, 4> result(<a float* with enough space>, <size0>, ...) = t4;
00231     cout << result(0, 0, 0);
00232 
00233     TensorFixedSize<float, Sizes<size0, ...>> result = t4;
00234     cout << result(0, 0, 0);
00235 
00236 Until you need the results, you can keep the operation around, and even reuse
00237 it for additional operations.  As long as you keep the expression as an
00238 operation, no computation is performed.
00239 
00240     // One way to compute exp((t1 + t2) * 0.2f);
00241     auto t3 = t1 + t2;
00242     auto t4 = t3 * 0.2f;
00243     auto t5 = t4.exp();
00244     Tensor<float, 3> result = t5;
00245 
00246     // Another way, exactly as efficient as the previous one:
00247     Tensor<float, 3> result = ((t1 + t2) * 0.2f).exp();
00248 
00249 ### Controlling When Expression are Evaluated
00250 
00251 There are several ways to control when expressions are evaluated:
00252 
00253 *   Assignment to a Tensor, TensorFixedSize, or TensorMap.
00254 *   Use of the eval() method.
00255 *   Assignment to a TensorRef.
00256 
00257 #### Assigning to a Tensor, TensorFixedSize, or TensorMap.
00258 
00259 The most common way to evaluate an expression is to assign it to a Tensor.  In
00260 the example below, the ```auto``` declarations make the intermediate values
00261 "Operations", not Tensors, and do not cause the expressions to be evaluated.
00262 The assignment to the Tensor ```result``` causes the evaluation of all the
00263 operations.
00264 
00265     auto t3 = t1 + t2;             // t3 is an Operation.
00266     auto t4 = t3 * 0.2f;           // t4 is an Operation.
00267     auto t5 = t4.exp();            // t5 is an Operation.
00268     Tensor<float, 3> result = t5;  // The operations are evaluated.
00269 
00270 If you know the ranks and sizes of the Operation value you can assign the
00271 Operation to a TensorFixedSize instead of a Tensor, which is a bit more
00272 efficient.
00273 
00274     // We know that the result is a 4x4x2 tensor!
00275     TensorFixedSize<float, 4, 4, 2> result = t5;
00276 
00277 Simiarly, assigning an expression to a TensorMap causes its evaluation.  Like
00278 tensors of type TensorFixedSize, TensorMaps cannot be resized so they have to
00279 have the rank and sizes of the expression that are assigned to them.
00280 
00281 #### Calling eval().
00282 
00283 When you compute large composite expressions, you sometimes want to tell Eigen
00284 that an intermediate value in the expression tree is worth evaluating ahead of
00285 time.  This is done by inserting a call to the ```eval()``` method of the
00286 expression Operation.
00287 
00288     // The previous example could have been written:
00289     Tensor<float, 3> result = ((t1 + t2) * 0.2f).exp();
00290 
00291     // If you want to compute (t1 + t2) once ahead of time you can write:
00292     Tensor<float, 3> result = ((t1 + t2).eval() * 0.2f).exp();
00293 
00294 Semantically, calling ```eval()``` is equivalent to materializing the value of
00295 the expression in a temporary Tensor of the right size.  The code above in
00296 effect does:
00297 
00298     // .eval() knows the size!
00299     TensorFixedSize<float, 4, 4, 2> tmp = t1 + t2;
00300     Tensor<float, 3> result = (tmp * 0.2f).exp();
00301 
00302 Note that the return value of ```eval()``` is itself an Operation, so the
00303 following code does not do what you may think:
00304 
00305     // Here t3 is an evaluation Operation.  t3 has not been evaluated yet.
00306     auto t3 = (t1 + t2).eval();
00307 
00308     // You can use t3 in another expression.  Still no evaluation.
00309     auto t4 = (t3 * 0.2f).exp();
00310 
00311     // The value is evaluated when you assign the Operation to a Tensor, using
00312     // an intermediate tensor to represent t3.x
00313     Tensor<float, 3> result = t4;
00314 
00315 While in the examples above calling ```eval()``` does not make a difference in
00316 performance, in other cases it can make a huge difference.  In the expression
00317 below the ```broadcast()``` expression causes the ```X.maximum()``` expression
00318 to be evaluated many times:
00319 
00320     Tensor<...> X ...;
00321     Tensor<...> Y = ((X - X.maximum(depth\_dim).reshape(dims2d).broadcast(bcast))
00322                      * beta).exp();
00323 
00324 Inserting a call to ```eval()``` between the ```maximum()``` and
00325 ```reshape()``` calls guarantees that maximum() is only computed once and
00326 greatly speeds-up execution:
00327 
00328     Tensor<...> Y =
00329       ((X - X.maximum(depth\_dim).eval().reshape(dims2d).broadcast(bcast))
00330         * beta).exp();
00331 
00332 In the other example below, the tensor ```Y``` is both used in the expression
00333 and its assignment.  This is an aliasing problem and if the evaluation is not
00334 done in the right order Y will be updated incrementally during the evaluation
00335 resulting in bogus results:
00336 
00337      Tensor<...> Y ...;
00338      Y = Y / (Y.sum(depth\_dim).reshape(dims2d).broadcast(bcast));
00339 
00340 Inserting a call to ```eval()``` between the ```sum()``` and ```reshape()```
00341 expressions ensures that the sum is computed before any updates to ```Y``` are
00342 done.
00343 
00344      Y = Y / (Y.sum(depth\_dim).eval().reshape(dims2d).broadcast(bcast));
00345 
00346 Note that an eval around the full right hand side expression is not needed
00347 because the generated has to compute the i-th value of the right hand side
00348 before assigning it to the left hand side.
00349 
00350 However, if you were assigning the expression value to a shuffle of ```Y```
00351 then you would need to force an eval for correctness by adding an ```eval()```
00352 call for the right hand side:
00353 
00354      Y.shuffle(...) =
00355         (Y / (Y.sum(depth\_dim).eval().reshape(dims2d).broadcast(bcast))).eval();
00356 
00357 
00358 #### Assigning to a TensorRef.
00359 
00360 If you need to access only a few elements from the value of an expression you
00361 can avoid materializing the value in a full tensor by using a TensorRef.
00362 
00363 A TensorRef is a small wrapper class for any Eigen Operation.  It provides
00364 overloads for the ```()``` operator that let you access individual values in
00365 the expression.  TensorRef is convenient, because the Operation themselves do
00366 not provide a way to access individual elements.
00367 
00368     // Create a TensorRef for the expression.  The expression is not
00369     // evaluated yet.
00370     TensorRef<Tensor<float, 3> > ref = ((t1 + t2) * 0.2f).exp();
00371 
00372     // Use "ref" to access individual elements.  The expression is evaluated
00373     // on the fly.
00374     float at\_0 = ref(0, 0, 0);
00375     cout << ref(0, 1, 0);
00376 
00377 Only use TensorRef when you need a subset of the values of the expression.
00378 TensorRef only computes the values you access.  However note that if you are
00379 going to access all the values it will be much faster to materialize the
00380 results in a Tensor first.
00381 
00382 In some cases, if the full Tensor result would be very large, you may save
00383 memory by accessing it as a TensorRef.  But not always.  So don't count on it.
00384 
00385 
00386 ### Controlling How Expressions Are Evaluated
00387 
00388 The tensor library provides several implementations of the various operations
00389 such as contractions and convolutions.  The implementations are optimized for
00390 different environments: single threaded on CPU, multi threaded on CPU, or on a
00391 GPU using cuda.  Additional implementations may be added later.
00392 
00393 You can choose which implementation to use with the ```device()``` call.  If
00394 you do not choose an implementation explicitly the default implementation that
00395 uses a single thread on the CPU is used.
00396 
00397 The default implementation has been optimized for recent Intel CPUs, taking
00398 advantage of SSE, AVX, and FMA instructions.  Work is ongoing to tune the
00399 library on ARM CPUs.  Note that you need to pass compiler-dependent flags
00400 to enable the use of SSE, AVX, and other instructions.
00401 
00402 For example, the following code adds two tensors using the default
00403 single-threaded CPU implementation:
00404 
00405     Tensor<float, 2> a(30, 40);
00406     Tensor<float, 2> b(30, 40);
00407     Tensor<float, 2> c = a + b;
00408 
00409 To choose a different implementation you have to insert a ```device()``` call
00410 before the assignment of the result.  For technical C++ reasons this requires
00411 that the Tensor for the result be declared on its own.  This means that you
00412 have to know the size of the result.
00413 
00414     Eigen::Tensor<float, 2> c(30, 40);
00415     c.device(...) = a + b;
00416 
00417 The call to ```device()``` must be the last call on the left of the operator=.
00418 
00419 You must pass to the ```device()``` call an Eigen device object.  There are
00420 presently three devices you can use: DefaultDevice, ThreadPoolDevice and
00421 GpuDevice.
00422 
00423 
00424 #### Evaluating With the DefaultDevice
00425 
00426 This is exactly the same as not inserting a ```device()``` call.
00427 
00428     DefaultDevice my\_device;
00429     c.device(my\_device) = a + b;
00430 
00431 #### Evaluating with a Thread Pool
00432 
00433     // Create the Eigen ThreadPoolDevice.
00434     Eigen::ThreadPoolDevice my\_device(4 /* number of threads to use */);
00435 
00436     // Now just use the device when evaluating expressions.
00437     Eigen::Tensor<float, 2> c(30, 50);
00438     c.device(my\_device) = a.contract(b, dot\_product\_dims);
00439 
00440 
00441 #### Evaluating On GPU
00442 
00443 This is presently a bit more complicated than just using a thread pool device.
00444 You need to create a GPU device but you also need to explicitly allocate the
00445 memory for tensors with cuda.
00446 
00447 
00448 ## API Reference
00449 
00450 ### Datatypes
00451 
00452 In the documentation of the tensor methods and Operation we mention datatypes
00453 that are tensor-type specific:
00454 
00455 #### <Tensor-Type>::Dimensions
00456 
00457 Acts like an array of ints.  Has an ```int size``` attribute, and can be
00458 indexed like an array to access individual values.  Used to represent the
00459 dimensions of a tensor.  See ```dimensions()```.
00460 
00461 #### <Tensor-Type>::Index
00462 
00463 Acts like an ```int```.  Used for indexing tensors along their dimensions.  See
00464 ```operator()```, ```dimension()```, and ```size()```.
00465 
00466 #### <Tensor-Type>::Scalar
00467 
00468 Represents the datatype of individual tensor elements.  For example, for a
00469 ```Tensor<float>```, ```Scalar``` is the type ```float```.  See
00470 ```setConstant()```.
00471 
00472 #### <Operation>
00473 
00474 We use this pseudo type to indicate that a tensor Operation is returned by a
00475 method.  We indicate in the text the type and dimensions of the tensor that the
00476 Operation returns after evaluation.
00477 
00478 The Operation will have to be evaluated, for example by assigning it to a
00479 tensor, before you can access the values of the resulting tensor.  You can also
00480 access the values through a TensorRef.
00481 
00482 
00483 ## Built-in Tensor Methods
00484 
00485 These are usual C++ methods that act on tensors immediately.  They are not
00486 Operations which provide delayed evaluation of their results.  Unless specified
00487 otherwise, all the methods listed below are available on all tensor classes:
00488 Tensor, TensorFixedSize, and TensorMap.
00489 
00490 ## Metadata
00491 
00492 ### int NumDimensions
00493 
00494 Constant value indicating the number of dimensions of a Tensor.  This is also
00495 known as the tensor "rank".
00496 
00497       Eigen::Tensor<float, 2> a(3, 4);
00498       cout << "Dims " << a.NumDimensions;
00499       => Dims 2
00500 
00501 ### Dimensions dimensions()
00502 
00503 Returns an array-like object representing the dimensions of the tensor.
00504 The actual type of the dimensions() result is <Tensor-Type>::Dimensions.
00505 
00506     Eigen::Tensor<float, 2> a(3, 4);
00507     const Eigen::Tensor<float, 2>::Dimensions& d = a.dimensions();
00508     cout << "Dim size: " << d.size << ", dim 0: " << d[0]
00509          << ", dim 1: " << d[1];
00510     => Dim size: 2, dim 0: 3, dim 1: 4
00511 
00512 If you use a C++11 compiler, you can use ```auto``` to simplify the code:
00513 
00514     const auto& d = a.dimensions();
00515     cout << "Dim size: " << d.size << ", dim 0: " << d[0]
00516          << ", dim 1: " << d[1];
00517     => Dim size: 2, dim 0: 3, dim 1: 4
00518 
00519 ### Index dimension(Index n)
00520 
00521 Returns the n-th dimension of the tensor.  The actual type of the
00522 ```dimension()``` result is ```<Tensor-Type>::Index```, but you can
00523 always use it like an int.
00524 
00525       Eigen::Tensor<float, 2> a(3, 4);
00526       int dim1 = a.dimension(1);
00527       cout << "Dim 1: " << dim1;
00528       => Dim 1: 4
00529 
00530 ### Index size()
00531 
00532 Returns the total number of elements in the tensor.  This is the product of all
00533 the tensor dimensions.  The actual type of the ```size()``` result is
00534 ```<Tensor-Type>::Index```, but you can always use it like an int.
00535 
00536     Eigen::Tensor<float, 2> a(3, 4);
00537     cout << "Size: " << a.size();
00538     => Size: 12
00539 
00540 
00541 ### Getting Dimensions From An Operation
00542 
00543 A few operations provide ```dimensions()``` directly,
00544 e.g. ```TensorReslicingOp```.  Most operations defer calculating dimensions
00545 until the operation is being evaluated.  If you need access to the dimensions
00546 of a deferred operation, you can wrap it in a TensorRef (see Assigning to a
00547 TensorRef above), which provides ```dimensions()``` and ```dimension()``` as
00548 above.
00549 
00550 TensorRef can also wrap the plain Tensor types, so this is a useful idiom in
00551 templated contexts where the underlying object could be either a raw Tensor
00552 or some deferred operation (e.g. a slice of a Tensor).  In this case, the
00553 template code can wrap the object in a TensorRef and reason about its
00554 dimensionality while remaining agnostic to the underlying type.
00555 
00556 
00557 ## Constructors
00558 
00559 ### Tensor
00560 
00561 Creates a tensor of the specified size. The number of arguments must be equal
00562 to the rank of the tensor. The content of the tensor is not initialized.
00563 
00564     Eigen::Tensor<float, 2> a(3, 4);
00565     cout << "NumRows: " << a.dimension(0) << " NumCols: " << a.dimension(1) << endl;
00566     => NumRows: 3 NumCols: 4
00567 
00568 ### TensorFixedSize
00569 
00570 Creates a tensor of the specified size. The number of arguments in the Size<>
00571 template parameter determines the rank of the tensor. The content of the tensor
00572 is not initialized.
00573 
00574     Eigen::TensorFixedSize<float, Size<3, 4>> a;
00575     cout << "Rank: " << a.rank() << endl;
00576     => Rank: 2
00577     cout << "NumRows: " << a.dimension(0) << " NumCols: " << a.dimension(1) << endl;
00578     => NumRows: 3 NumCols: 4
00579 
00580 ### TensorMap
00581 
00582 Creates a tensor mapping an existing array of data. The data must not be freed
00583 until the TensorMap is discarded, and the size of the data must be large enough
00584 to accomodate of the coefficients of the tensor.
00585 
00586     float data[] = \{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11\};
00587     Eigen::TensorMap<float, 2> a(data, 3, 4);
00588     cout << "NumRows: " << a.dimension(0) << " NumCols: " << a.dimension(1) << endl;
00589     => NumRows: 3 NumCols: 4
00590     cout << "a(1, 2): " << a(1, 2) << endl;
00591     => a(1, 2): 9
00592 
00593 
00594 ## Contents Initialization
00595 
00596 When a new Tensor or a new TensorFixedSize are created, memory is allocated to
00597 hold all the tensor elements, but the memory is not initialized.  Similarly,
00598 when a new TensorMap is created on top of non-initialized memory the memory its
00599 contents are not initialized.
00600 
00601 You can use one of the methods below to initialize the tensor memory.  These
00602 have an immediate effect on the tensor and return the tensor itself as a
00603 result.  These are not tensor Operations which delay evaluation.
00604 
00605 ### <Tensor-Type> setConstant(const Scalar& val)
00606 
00607 Sets all elements of the tensor to the constant value ```val```.  ```Scalar```
00608 is the type of data stored in the tensor.  You can pass any value that is
00609 convertible to that type.
00610 
00611 Returns the tensor itself in case you want to chain another call.
00612 
00613     a.setConstant(12.3f);
00614     cout << "Constant: " << endl << a << endl << endl;
00615     =>
00616     Constant:
00617     12.3 12.3 12.3 12.3
00618     12.3 12.3 12.3 12.3
00619     12.3 12.3 12.3 12.3
00620 
00621 Note that ```setConstant()``` can be used on any tensor where the element type
00622 has a copy constructor and an ```operator=()```:
00623 
00624     Eigen::Tensor<string, 2> a(2, 3);
00625     a.setConstant("yolo");
00626     cout << "String tensor: " << endl << a << endl << endl;
00627     =>
00628     String tensor:
00629     yolo yolo yolo
00630     yolo yolo yolo
00631 
00632 
00633 ### <Tensor-Type> setZero()
00634 
00635 Fills the tensor with zeros.  Equivalent to ```setConstant(Scalar(0))```.
00636 Returns the tensor itself in case you want to chain another call.
00637 
00638     a.setZero();
00639     cout << "Zeros: " << endl << a << endl << endl;
00640     =>
00641     Zeros:
00642     0 0 0 0
00643     0 0 0 0
00644     0 0 0 0
00645 
00646 
00647 ### <Tensor-Type> setValues(\{..initializer\_list\})
00648 
00649 Fills the tensor with explicit values specified in a std::initializer\_list.
00650 The type of the initializer list depends on the type and rank of the tensor.
00651 
00652 If the tensor has rank N, the initializer list must be nested N times.  The
00653 most deeply nested lists must contains P scalars of the Tensor type where P is
00654 the size of the last dimension of the Tensor.
00655 
00656 For example, for a ```TensorFixedSize<float, 2, 3>``` the initializer list must
00657 contains 2 lists of 3 floats each.
00658 
00659 ```setValues()``` returns the tensor itself in case you want to chain another
00660 call.
00661 
00662     Eigen::Tensor<float, 2> a(2, 3);
00663     a.setValues(\{\{0.0f, 1.0f, 2.0f\}, \{3.0f, 4.0f, 5.0f\}\});
00664     cout << "a" << endl << a << endl << endl;
00665     =>
00666     a
00667     0 1 2
00668     3 4 5
00669 
00670 If a list is too short, the corresponding elements of the tensor will not be
00671 changed.  This is valid at each level of nesting.  For example the following
00672 code only sets the values of the first row of the tensor.
00673 
00674     Eigen::Tensor<int, 2> a(2, 3);
00675     a.setConstant(1000);
00676     a.setValues(\{\{10, 20, 30\}\});
00677     cout << "a" << endl << a << endl << endl;
00678     =>
00679     a
00680     10   20   30
00681     1000 1000 1000
00682 
00683 ### <Tensor-Type> setRandom()
00684 
00685 Fills the tensor with random values.  Returns the tensor itself in case you
00686 want to chain another call.
00687 
00688     a.setRandom();
00689     cout << "Random: " << endl << a << endl << endl;
00690     =>
00691     Random:
00692       0.680375    0.59688  -0.329554    0.10794
00693      -0.211234   0.823295   0.536459 -0.0452059
00694       0.566198  -0.604897  -0.444451   0.257742
00695 
00696 You can customize ```setRandom()``` by providing your own random number
00697 generator as a template argument:
00698 
00699     a.setRandom<MyRandomGenerator>();
00700 
00701 Here, ```MyRandomGenerator``` must be a struct with the following member
00702 functions, where Scalar and Index are the same as ```<Tensor-Type>::Scalar```
00703 and ```<Tensor-Type>::Index```.
00704 
00705 See ```struct UniformRandomGenerator``` in TensorFunctors.h for an example.
00706 
00707     // Custom number generator for use with setRandom().
00708     struct MyRandomGenerator \{
00709       // Default and copy constructors. Both are needed
00710       MyRandomGenerator() \{ \}
00711       MyRandomGenerator(const MyRandomGenerator& ) \{ \}
00712 
00713       // Return a random value to be used.  "element\_location" is the
00714       // location of the entry to set in the tensor, it can typically
00715       // be ignored.
00716       Scalar operator()(Eigen::DenseIndex element\_location,
00717                         Eigen::DenseIndex /*unused*/ = 0) const \{
00718         return <randomly generated value of type T>;
00719       \}
00720 
00721       // Same as above but generates several numbers at a time.
00722       typename internal::packet\_traits<Scalar>::type packetOp(
00723           Eigen::DenseIndex packet\_location, Eigen::DenseIndex /*unused*/ = 0) const \{
00724         return <a packet of randomly generated values>;
00725       \}
00726     \};
00727 
00728 You can also use one of the 2 random number generators that are part of the
00729 tensor library:
00730 *   UniformRandomGenerator
00731 *   NormalRandomGenerator
00732 
00733 
00734 ## Data Access
00735 
00736 The Tensor, TensorFixedSize, and TensorRef classes provide the following
00737 accessors to access the tensor coefficients:
00738 
00739     const Scalar& operator()(const array<Index, NumIndices>& indices)
00740     const Scalar& operator()(Index firstIndex, IndexTypes... otherIndices)
00741     Scalar& operator()(const array<Index, NumIndices>& indices)
00742     Scalar& operator()(Index firstIndex, IndexTypes... otherIndices)
00743 
00744 The number of indices must be equal to the rank of the tensor. Moreover, these
00745 accessors are not available on tensor expressions. In order to access the
00746 values of a tensor expression, the expression must either be evaluated or
00747 wrapped in a TensorRef.
00748 
00749 
00750 ### Scalar* data() and const Scalar* data() const
00751 
00752 Returns a pointer to the storage for the tensor.  The pointer is const if the
00753 tensor was const.  This allows direct access to the data.  The layout of the
00754 data depends on the tensor layout: RowMajor or ColMajor.
00755 
00756 This access is usually only needed for special cases, for example when mixing
00757 Eigen Tensor code with other libraries.
00758 
00759 Scalar is the type of data stored in the tensor.
00760 
00761     Eigen::Tensor<float, 2> a(3, 4);
00762     float* a\_data = a.data();
00763     a\_data[0] = 123.45f;
00764     cout << "a(0, 0): " << a(0, 0);
00765     => a(0, 0): 123.45
00766 
00767 
00768 ## Tensor Operations
00769 
00770 All the methods documented below return non evaluated tensor ```Operations```.
00771 These can be chained: you can apply another Tensor Operation to the value
00772 returned by the method.
00773 
00774 The chain of Operation is evaluated lazily, typically when it is assigned to a
00775 tensor.  See "Controlling when Expression are Evaluated" for more details about
00776 their evaluation.
00777 
00778 ### <Operation> constant(const Scalar& val)
00779 
00780 Returns a tensor of the same type and dimensions as the original tensor but
00781 where all elements have the value ```val```.
00782 
00783 This is useful, for example, when you want to add or subtract a constant from a
00784 tensor, or multiply every element of a tensor by a scalar.
00785 
00786     Eigen::Tensor<float, 2> a(2, 3);
00787     a.setConstant(1.0f);
00788     Eigen::Tensor<float, 2> b = a + a.constant(2.0f);
00789     Eigen::Tensor<float, 2> c = b * b.constant(0.2f);
00790     cout << "a" << endl << a << endl << endl;
00791     cout << "b" << endl << b << endl << endl;
00792     cout << "c" << endl << c << endl << endl;
00793     =>
00794     a
00795     1 1 1
00796     1 1 1
00797 
00798     b
00799     3 3 3
00800     3 3 3
00801 
00802     c
00803     0.6 0.6 0.6
00804     0.6 0.6 0.6
00805 
00806 ### <Operation> random()
00807 
00808 Returns a tensor of the same type and dimensions as the current tensor
00809 but where all elements have random values.
00810 
00811 This is for example useful to add random values to an existing tensor.
00812 The generation of random values can be customized in the same manner
00813 as for ```setRandom()```.
00814 
00815     Eigen::Tensor<float, 2> a(2, 3);
00816     a.setConstant(1.0f);
00817     Eigen::Tensor<float, 2> b = a + a.random();
00818     cout << "a" << endl << a << endl << endl;
00819     cout << "b" << endl << b << endl << endl;
00820     =>
00821     a
00822     1 1 1
00823     1 1 1
00824 
00825     b
00826     1.68038   1.5662  1.82329
00827     0.788766  1.59688 0.395103
00828 
00829 
00830 ## Unary Element Wise Operations
00831 
00832 All these operations take a single input tensor as argument and return a tensor
00833 of the same type and dimensions as the tensor to which they are applied.  The
00834 requested operations are applied to each element independently.
00835 
00836 ### <Operation> operator-()
00837 
00838 Returns a tensor of the same type and dimensions as the original tensor
00839 containing the opposite values of the original tensor.
00840 
00841     Eigen::Tensor<float, 2> a(2, 3);
00842     a.setConstant(1.0f);
00843     Eigen::Tensor<float, 2> b = -a;
00844     cout << "a" << endl << a << endl << endl;
00845     cout << "b" << endl << b << endl << endl;
00846     =>
00847     a
00848     1 1 1
00849     1 1 1
00850 
00851     b
00852     -1 -1 -1
00853     -1 -1 -1
00854 
00855 ### <Operation> sqrt()
00856 
00857 Returns a tensor of the same type and dimensions as the original tensor
00858 containing the square roots of the original tensor.
00859 
00860 ### <Operation> rsqrt()
00861 
00862 Returns a tensor of the same type and dimensions as the original tensor
00863 containing the inverse square roots of the original tensor.
00864 
00865 ### <Operation> square()
00866 
00867 Returns a tensor of the same type and dimensions as the original tensor
00868 containing the squares of the original tensor values.
00869 
00870 ### <Operation> inverse()
00871 
00872 Returns a tensor of the same type and dimensions as the original tensor
00873 containing the inverse of the original tensor values.
00874 
00875 ### <Operation> exp()
00876 
00877 Returns a tensor of the same type and dimensions as the original tensor
00878 containing the exponential of the original tensor.
00879 
00880 ### <Operation> log()
00881 
00882 Returns a tensor of the same type and dimensions as the original tensor
00883 containing the natural logarithms of the original tensor.
00884 
00885 ### <Operation> abs()
00886 
00887 Returns a tensor of the same type and dimensions as the original tensor
00888 containing the absolute values of the original tensor.
00889 
00890 ### <Operation> pow(Scalar exponent)
00891 
00892 Returns a tensor of the same type and dimensions as the original tensor
00893 containing the coefficients of the original tensor to the power of the
00894 exponent.
00895 
00896 The type of the exponent, Scalar, is always the same as the type of the
00897 tensor coefficients.  For example, only integer exponents can be used in
00898 conjuntion with tensors of integer values.
00899 
00900 You can use cast() to lift this restriction.  For example this computes
00901 cubic roots of an int Tensor:
00902 
00903     Eigen::Tensor<int, 2> a(2, 3);
00904     a.setValues(\{\{0, 1, 8\}, \{27, 64, 125\}\});
00905     Eigen::Tensor<double, 2> b = a.cast<double>().pow(1.0 / 3.0);
00906     cout << "a" << endl << a << endl << endl;
00907     cout << "b" << endl << b << endl << endl;
00908     =>
00909     a
00910     0   1   8
00911     27  64 125
00912 
00913     b
00914     0 1 2
00915     3 4 5
00916 
00917 ### <Operation>  operator * (Scalar scale)
00918 
00919 Multiplies all the coefficients of the input tensor by the provided scale.
00920 
00921 ### <Operation>  cwiseMax(Scalar threshold)
00922 TODO
00923 
00924 ### <Operation>  cwiseMin(Scalar threshold)
00925 TODO
00926 
00927 ### <Operation>  unaryExpr(const CustomUnaryOp& func)
00928 TODO
00929 
00930 
00931 ## Binary Element Wise Operations
00932 
00933 These operations take two input tensors as arguments. The 2 input tensors should
00934 be of the same type and dimensions. The result is a tensor of the same
00935 dimensions as the tensors to which they are applied, and unless otherwise
00936 specified it is also of the same type. The requested operations are applied to
00937 each pair of elements independently.
00938 
00939 ### <Operation> operator+(const OtherDerived& other)
00940 
00941 Returns a tensor of the same type and dimensions as the input tensors
00942 containing the coefficient wise sums of the inputs.
00943 
00944 ### <Operation> operator-(const OtherDerived& other)
00945 
00946 Returns a tensor of the same type and dimensions as the input tensors
00947 containing the coefficient wise differences of the inputs.
00948 
00949 ### <Operation> operator*(const OtherDerived& other)
00950 
00951 Returns a tensor of the same type and dimensions as the input tensors
00952 containing the coefficient wise products of the inputs.
00953 
00954 ### <Operation> operator/(const OtherDerived& other)
00955 
00956 Returns a tensor of the same type and dimensions as the input tensors
00957 containing the coefficient wise quotients of the inputs.
00958 
00959 This operator is not supported for integer types.
00960 
00961 ### <Operation> cwiseMax(const OtherDerived& other)
00962 
00963 Returns a tensor of the same type and dimensions as the input tensors
00964 containing the coefficient wise maximums of the inputs.
00965 
00966 ### <Operation> cwiseMin(const OtherDerived& other)
00967 
00968 Returns a tensor of the same type and dimensions as the input tensors
00969 containing the coefficient wise mimimums of the inputs.
00970 
00971 ### <Operation> Logical operators
00972 
00973 The following logical operators are supported as well:
00974 
00975 *   operator&&(const OtherDerived& other)
00976 *   operator||(const OtherDerived& other)
00977 *   operator<(const OtherDerived& other)
00978 *   operator<=(const OtherDerived& other)
00979 *   operator>(const OtherDerived& other)
00980 *   operator>=(const OtherDerived& other)
00981 *   operator==(const OtherDerived& other)
00982 *   operator!=(const OtherDerived& other)
00983 
00984 They all return a tensor of boolean values.
00985 
00986 
00987 ## Selection (select(const ThenDerived& thenTensor, const ElseDerived& elseTensor)
00988 
00989 Selection is a coefficient-wise ternary operator that is the tensor equivalent
00990 to the if-then-else operation.
00991 
00992     Tensor<bool, 3> if = ...;
00993     Tensor<float, 3> then = ...;
00994     Tensor<float, 3> else = ...;
00995     Tensor<float, 3> result = if.select(then, else);
00996 
00997 The 3 arguments must be of the same dimensions, which will also be the dimension
00998 of the result.  The 'if' tensor must be of type boolean, the 'then' and the
00999 'else' tensor must be of the same type, which will also be the type of the
01000 result.
01001 
01002 Each coefficient in the result is equal to the corresponding coefficient in the
01003 'then' tensor if the corresponding value in the 'if' tensor is true. If not, the
01004 resulting coefficient will come from the 'else' tensor.
01005 
01006 
01007 ## Contraction
01008 
01009 Tensor *contractions* are a generalization of the matrix product to the
01010 multidimensional case.
01011 
01012     // Create 2 matrices using tensors of rank 2
01013     Eigen::Tensor<int, 2> a(2, 3);
01014     a.setValues(\{\{1, 2, 3\}, \{6, 5, 4\}\});
01015     Eigen::Tensor<int, 2> b(3, 2);
01016     a.setValues(\{\{1, 2\}, \{4, 5\}, \{5, 6\}\});
01017 
01018     // Compute the traditional matrix product
01019     array<IndexPair<int>, 1> product\_dims = \{ IndexPair(1, 0) \};
01020     Eigen::Tensor<int, 2> AB = a.contract(b, product\_dims);
01021 
01022     // Compute the product of the transpose of the matrices
01023     array<IndexPair<int>, 1> transpose\_product\_dims = \{ IndexPair(0, 1) \};
01024     Eigen::Tensor<int, 2> AtBt = a.contract(b, transposed\_product\_dims);
01025 
01026 
01027 ## Reduction Operations
01028 
01029 A *Reduction* operation returns a tensor with fewer dimensions than the
01030 original tensor.  The values in the returned tensor are computed by applying a
01031 *reduction operator* to slices of values from the original tensor.  You specify
01032 the dimensions along which the slices are made.
01033 
01034 The Eigen Tensor library provides a set of predefined reduction operators such
01035 as ```maximum()``` and ```sum()``` and lets you define additional operators by
01036 implementing a few methods from a reductor template.
01037 
01038 ### Reduction Dimensions
01039 
01040 All reduction operations take a single parameter of type
01041 ```<TensorType>::Dimensions``` which can always be specified as an array of
01042 ints.  These are called the "reduction dimensions."  The values are the indices
01043 of the dimensions of the input tensor over which the reduction is done.  The
01044 parameter can have at most as many element as the rank of the input tensor;
01045 each element must be less than the tensor rank, as it indicates one of the
01046 dimensions to reduce.
01047 
01048 Each dimension of the input tensor should occur at most once in the reduction
01049 dimensions as the implementation does not remove duplicates.
01050 
01051 The order of the values in the reduction dimensions does not affect the
01052 results, but the code may execute faster if you list the dimensions in
01053 increasing order.
01054 
01055 Example: Reduction along one dimension.
01056 
01057     // Create a tensor of 2 dimensions
01058     Eigen::Tensor<int, 2> a(2, 3);
01059     a.setValues(\{\{1, 2, 3\}, \{6, 5, 4\}\});
01060     // Reduce it along the second dimension (1)...
01061     Eigen::array<int, 1> dims(\{1 /* dimension to reduce */\});
01062     // ...using the "maximum" operator.
01063     // The result is a tensor with one dimension.  The size of
01064     // that dimension is the same as the first (non-reduced) dimension of a.
01065     Eigen::Tensor<int, 1> b = a.maximum(dims);
01066     cout << "a" << endl << a << endl << endl;
01067     cout << "b" << endl << b << endl << endl;
01068     =>
01069     a
01070     1 2 3
01071     6 5 4
01072 
01073     b
01074     3
01075     6
01076 
01077 Example: Reduction along two dimensions.
01078 
01079     Eigen::Tensor<float, 3, Eigen::ColMajor> a(2, 3, 4);
01080     a.setValues(\{\{\{0.0f, 1.0f, 2.0f, 3.0f\},
01081                   \{7.0f, 6.0f, 5.0f, 4.0f\},
01082                   \{8.0f, 9.0f, 10.0f, 11.0f\}\},
01083                  \{\{12.0f, 13.0f, 14.0f, 15.0f\},
01084                   \{19.0f, 18.0f, 17.0f, 16.0f\},
01085                   \{20.0f, 21.0f, 22.0f, 23.0f\}\}\});
01086     // The tensor a has 3 dimensions.  We reduce along the
01087     // first 2, resulting in a tensor with a single dimension
01088     // of size 4 (the last dimension of a.)
01089     // Note that we pass the array of reduction dimensions
01090     // directly to the maximum() call.
01091     Eigen::Tensor<float, 1, Eigen::ColMajor> b =
01092         a.maximum(Eigen::array<int, 2>(\{0, 1\}));
01093     cout << "b" << endl << b << endl << endl;
01094     =>
01095     b
01096     20
01097     21
01098     22
01099     23
01100 
01101 #### Reduction along all dimensions
01102 
01103 As a special case, if you pass no parameter to a reduction operation the
01104 original tensor is reduced along *all* its dimensions.  The result is a
01105 scalar, represented as a zero-dimension tensor.
01106 
01107     Eigen::Tensor<float, 3> a(2, 3, 4);
01108     a.setValues(\{\{\{0.0f, 1.0f, 2.0f, 3.0f\},
01109                   \{7.0f, 6.0f, 5.0f, 4.0f\},
01110                   \{8.0f, 9.0f, 10.0f, 11.0f\}\},
01111                  \{\{12.0f, 13.0f, 14.0f, 15.0f\},
01112                   \{19.0f, 18.0f, 17.0f, 16.0f\},
01113                   \{20.0f, 21.0f, 22.0f, 23.0f\}\}\});
01114     // Reduce along all dimensions using the sum() operator.
01115     Eigen::Tensor<float, 0> b = a.sum();
01116     cout << "b" << endl << b << endl << endl;
01117     =>
01118     b
01119     276
01120 
01121 
01122 ### <Operation> sum(const Dimensions& new\_dims)
01123 ### <Operation> sum()
01124 
01125 Reduce a tensor using the sum() operator.  The resulting values
01126 are the sum of the reduced values.
01127 
01128 ### <Operation> mean(const Dimensions& new\_dims)
01129 ### <Operation> mean()
01130 
01131 Reduce a tensor using the mean() operator.  The resulting values
01132 are the mean of the reduced values.
01133 
01134 ### <Operation> maximum(const Dimensions& new\_dims)
01135 ### <Operation> maximum()
01136 
01137 Reduce a tensor using the maximum() operator.  The resulting values are the
01138 largest of the reduced values.
01139 
01140 ### <Operation> minimum(const Dimensions& new\_dims)
01141 ### <Operation> minimum()
01142 
01143 Reduce a tensor using the minimum() operator.  The resulting values
01144 are the smallest of the reduced values.
01145 
01146 ### <Operation> prod(const Dimensions& new\_dims)
01147 ### <Operation> prod()
01148 
01149 Reduce a tensor using the prod() operator.  The resulting values
01150 are the product of the reduced values.
01151 
01152 ### <Operation> all(const Dimensions& new\_dims)
01153 ### <Operation> all()
01154 Reduce a tensor using the all() operator.  Casts tensor to bool and then checks
01155 whether all elements are true.  Runs through all elements rather than
01156 short-circuiting, so may be significantly inefficient.
01157 
01158 ### <Operation> any(const Dimensions& new\_dims)
01159 ### <Operation> any()
01160 Reduce a tensor using the any() operator.  Casts tensor to bool and then checks
01161 whether any element is true.  Runs through all elements rather than
01162 short-circuiting, so may be significantly inefficient.
01163 
01164 
01165 ### <Operation> reduce(const Dimensions& new\_dims, const Reducer& reducer)
01166 
01167 Reduce a tensor using a user-defined reduction operator.  See ```SumReducer```
01168 in TensorFunctors.h for information on how to implement a reduction operator.
01169 
01170 
01171 ## Scan Operations
01172 
01173 A *Scan* operation returns a tensor with the same dimensions as the original
01174 tensor. The operation performs an inclusive scan along the specified
01175 axis, which means it computes a running total along the axis for a given
01176 reduction operation.
01177 If the reduction operation corresponds to summation, then this computes the
01178 prefix sum of the tensor along the given axis.
01179 
01180 Example:
01181 dd a comment to this line
01182 
01183     // Create a tensor of 2 dimensions
01184     Eigen::Tensor<int, 2> a(2, 3);
01185     a.setValues(\{\{1, 2, 3\}, \{4, 5, 6\}\});
01186     // Scan it along the second dimension (1) using summation
01187     Eigen::Tensor<int, 2> b = a.cumsum(1);
01188     // The result is a tensor with the same size as the input
01189     cout << "a" << endl << a << endl << endl;
01190     cout << "b" << endl << b << endl << endl;
01191     =>
01192     a
01193     1 2 3
01194     6 5 4
01195 
01196     b
01197     1  3  6
01198     4  9 15
01199 
01200 ### <Operation> cumsum(const Index& axis)
01201 
01202 Perform a scan by summing consecutive entries.
01203 
01204 ### <Operation> cumprod(const Index& axis)
01205 
01206 Perform a scan by multiplying consecutive entries.
01207 
01208 
01209 ## Convolutions
01210 
01211 ### <Operation> convolve(const Kernel& kernel, const Dimensions& dims)
01212 
01213 Returns a tensor that is the output of the convolution of the input tensor with the kernel,
01214 along the specified dimensions of the input tensor. The dimension size for dimensions of the output
       tensor
01215 which were part of the convolution will be reduced by the formula:
01216 output\_dim\_size = input\_dim\_size - kernel\_dim\_size + 1 (requires: input\_dim\_size >= kernel\_dim\_size).
01217 The dimension sizes for dimensions that were not part of the convolution will remain the same.
01218 Performance of the convolution can depend on the length of the stride(s) of the input tensor
       dimension(s) along which the
01219 convolution is computed (the first dimension has the shortest stride for ColMajor, whereas RowMajor's
       shortest stride is
01220 for the last dimension).
01221 
01222     // Compute convolution along the second and third dimension.
01223     Tensor<float, 4, DataLayout> input(3, 3, 7, 11);
01224     Tensor<float, 2, DataLayout> kernel(2, 2);
01225     Tensor<float, 4, DataLayout> output(3, 2, 6, 11);
01226     input.setRandom();
01227     kernel.setRandom();
01228 
01229     Eigen::array<ptrdiff\_t, 2> dims(\{1, 2\});  // Specify second and third dimension for convolution.
01230     output = input.convolve(kernel, dims);
01231 
01232     for (int i = 0; i < 3; ++i) \{
01233       for (int j = 0; j < 2; ++j) \{
01234         for (int k = 0; k < 6; ++k) \{
01235           for (int l = 0; l < 11; ++l) \{
01236             const float result = output(i,j,k,l);
01237             const float expected = input(i,j+0,k+0,l) * kernel(0,0) +
01238                                    input(i,j+1,k+0,l) * kernel(1,0) +
01239                                    input(i,j+0,k+1,l) * kernel(0,1) +
01240                                    input(i,j+1,k+1,l) * kernel(1,1);
01241             VERIFY\_IS\_APPROX(result, expected);
01242           \}
01243         \}
01244       \}
01245     \}
01246 
01247 
01248 ## Geometrical Operations
01249 
01250 These operations return a Tensor with different dimensions than the original
01251 Tensor.  They can be used to access slices of tensors, see them with different
01252 dimensions, or pad tensors with additional data.
01253 
01254 ### <Operation> reshape(const Dimensions& new\_dims)
01255 
01256 Returns a view of the input tensor that has been reshaped to the specified
01257 new dimensions.  The argument new\_dims is an array of Index values.  The
01258 rank of the resulting tensor is equal to the number of elements in new\_dims.
01259 
01260 The product of all the sizes in the new dimension array must be equal to
01261 the number of elements in the input tensor.
01262 
01263     // Increase the rank of the input tensor by introducing a new dimension
01264     // of size 1.
01265     Tensor<float, 2> input(7, 11);
01266     array<int, 3> three\_dims\{\{7, 11, 1\}\};
01267     Tensor<float, 3> result = input.reshape(three\_dims);
01268 
01269     // Decrease the rank of the input tensor by merging 2 dimensions;
01270     array<int, 1> one\_dim\{\{7 * 11\}\};
01271     Tensor<float, 1> result = input.reshape(one\_dim);
01272 
01273 This operation does not move any data in the input tensor, so the resulting
01274 contents of a reshaped Tensor depend on the data layout of the original Tensor.
01275 
01276 For example this is what happens when you ```reshape()``` a 2D ColMajor tensor
01277 to one dimension:
01278 
01279     Eigen::Tensor<float, 2, Eigen::ColMajor> a(2, 3);
01280     a.setValues(\{\{0.0f, 100.0f, 200.0f\}, \{300.0f, 400.0f, 500.0f\}\});
01281     Eigen::array<Eigen::DenseIndex, 1> one\_dim(\{3 * 2\});
01282     Eigen::Tensor<float, 1, Eigen::ColMajor> b = a.reshape(one\_dim);
01283     cout << "b" << endl << b << endl;
01284     =>
01285     b
01286       0
01287     300
01288     100
01289     400
01290     200
01291     500
01292 
01293 This is what happens when the 2D Tensor is RowMajor:
01294 
01295     Eigen::Tensor<float, 2, Eigen::RowMajor> a(2, 3);
01296     a.setValues(\{\{0.0f, 100.0f, 200.0f\}, \{300.0f, 400.0f, 500.0f\}\});
01297     Eigen::array<Eigen::DenseIndex, 1> one\_dim(\{3 * 2\});
01298     Eigen::Tensor<float, 1, Eigen::RowMajor> b = a.reshape(one\_dim);
01299     cout << "b" << endl << b << endl;
01300     =>
01301     b
01302       0
01303     100
01304     200
01305     300
01306     400
01307     500
01308 
01309 The reshape operation is a lvalue. In other words, it can be used on the left
01310 side of the assignment operator.
01311 
01312 The previous example can be rewritten as follow:
01313 
01314     Eigen::Tensor<float, 2, Eigen::ColMajor> a(2, 3);
01315     a.setValues(\{\{0.0f, 100.0f, 200.0f\}, \{300.0f, 400.0f, 500.0f\}\});
01316     Eigen::array<Eigen::DenseIndex, 2> two\_dim(\{2, 3\});
01317     Eigen::Tensor<float, 1, Eigen::ColMajor> b;
01318     b.reshape(two\_dim) = a;
01319     cout << "b" << endl << b << endl;
01320     =>
01321     b
01322       0
01323     300
01324     100
01325     400
01326     200
01327     500
01328 
01329 Note that "b" itself was not reshaped but that instead the assignment is done to
01330 the reshape view of b.
01331 
01332 
01333 ### <Operation> shuffle(const Shuffle& shuffle)
01334 
01335 Returns a copy of the input tensor whose dimensions have been
01336 reordered according to the specified permutation. The argument shuffle
01337 is an array of Index values. Its size is the rank of the input
01338 tensor. It must contain a permutation of 0, 1, ..., rank - 1. The i-th
01339 dimension of the output tensor equals to the size of the shuffle[i]-th
01340 dimension of the input tensor. For example:
01341 
01342     // Shuffle all dimensions to the left by 1.
01343     Tensor<float, 3> input(20, 30, 50);
01344     // ... set some values in input.
01345     Tensor<float, 3> output = input.shuffle(\{1, 2, 0\})
01346 
01347     eigen\_assert(output.dimension(0) == 30);
01348     eigen\_assert(output.dimension(1) == 50);
01349     eigen\_assert(output.dimension(2) == 20);
01350 
01351 Indices into the output tensor are shuffled accordingly to formulate
01352 indices into the input tensor. For example, one can assert in the above
01353 code snippet that:
01354 
01355     eigen\_assert(output(3, 7, 11) == input(11, 3, 7));
01356 
01357 In general, one can assert that
01358 
01359     eigen\_assert(output(..., indices[shuffle[i]], ...) ==
01360                  input(..., indices[i], ...))
01361 
01362 The shuffle operation results in a lvalue, which means that it can be assigned
01363 to. In other words, it can be used on the left side of the assignment operator.
01364 
01365 Let's rewrite the previous example to take advantage of this feature:
01366 
01367     // Shuffle all dimensions to the left by 1.
01368     Tensor<float, 3> input(20, 30, 50);
01369     // ... set some values in input.
01370     Tensor<float, 3> output(30, 50, 20);
01371     output.shuffle(\{2, 0, 1\}) = input;
01372 
01373 
01374 ### <Operation> stride(const Strides& strides)
01375 
01376 Returns a view of the input tensor that strides (skips stride-1
01377 elements) along each of the dimensions.  The argument strides is an
01378 array of Index values.  The dimensions of the resulting tensor are
01379 ceil(input\_dimensions[i] / strides[i]).
01380 
01381 For example this is what happens when you ```stride()``` a 2D tensor:
01382 
01383     Eigen::Tensor<int, 2> a(4, 3);
01384     a.setValues(\{\{0, 100, 200\}, \{300, 400, 500\}, \{600, 700, 800\}, \{900, 1000, 1100\}\});
01385     Eigen::array<Eigen::DenseIndex, 2> strides(\{3, 2\});
01386     Eigen::Tensor<int, 2> b = a.stride(strides);
01387     cout << "b" << endl << b << endl;
01388     =>
01389     b
01390        0   200
01391      900  1100
01392 
01393 It is possible to assign a tensor to a stride:
01394     Tensor<float, 3> input(20, 30, 50);
01395     // ... set some values in input.
01396     Tensor<float, 3> output(40, 90, 200);
01397     output.stride(\{2, 3, 4\}) = input;
01398 
01399 
01400 ### <Operation> slice(const StartIndices& offsets, const Sizes& extents)
01401 
01402 Returns a sub-tensor of the given tensor. For each dimension i, the slice is
01403 made of the coefficients stored between offset[i] and offset[i] + extents[i] in
01404 the input tensor.
01405 
01406     Eigen::Tensor<int, 2> a(4, 3);
01407     a.setValues(\{\{0, 100, 200\}, \{300, 400, 500\},
01408                  \{600, 700, 800\}, \{900, 1000, 1100\}\});
01409     Eigen::array<int, 2> offsets = \{1, 0\};
01410     Eigen::array<int, 2> extents = \{2, 2\};
01411     Eigen::Tensor<int, 1> slice = a.slice(offsets, extents);
01412     cout << "a" << endl << a << endl;
01413     =>
01414     a
01415        0   100   200
01416      300   400   500
01417      600   700   800
01418      900  1000  1100
01419     cout << "slice" << endl << slice << endl;
01420     =>
01421     slice
01422      300   400
01423      600   700
01424 
01425 
01426 ### <Operation> chip(const Index offset, const Index dim)
01427 
01428 A chip is a special kind of slice. It is the subtensor at the given offset in
01429 the dimension dim. The returned tensor has one fewer dimension than the input
01430 tensor: the dimension dim is removed.
01431 
01432 For example, a matrix chip would be either a row or a column of the input
01433 matrix.
01434 
01435     Eigen::Tensor<int, 2> a(4, 3);
01436     a.setValues(\{\{0, 100, 200\}, \{300, 400, 500\},
01437                  \{600, 700, 800\}, \{900, 1000, 1100\}\});
01438     Eigen::Tensor<int, 1> row\_3 = a.chip(2, 0);
01439     Eigen::Tensor<int, 1> col\_2 = a.chip(1, 1);
01440     cout << "a" << endl << a << endl;
01441     =>
01442     a
01443        0   100   200
01444      300   400   500
01445      600   700   800
01446      900  1000  1100
01447     cout << "row\_3" << endl << row\_3 << endl;
01448     =>
01449     row\_3
01450        600   700   800
01451     cout << "col\_2" << endl << col\_2 << endl;
01452     =>
01453     col\_2
01454        100   400   700    1000
01455 
01456 It is possible to assign values to a tensor chip since the chip operation is a
01457 lvalue. For example:
01458 
01459     Eigen::Tensor<int, 1> a(3);
01460     a.setValues(\{\{100, 200, 300\}\});
01461     Eigen::Tensor<int, 2> b(2, 3);
01462     b.setZero();
01463     b.chip(0, 0) = a;
01464     cout << "a" << endl << a << endl;
01465     =>
01466     a
01467      100
01468      200
01469      300
01470     cout << "b" << endl << b << endl;
01471     =>
01472     b
01473        100   200   300
01474          0     0     0
01475 
01476 
01477 ### <Operation> reverse(const ReverseDimensions& reverse)
01478 
01479 Returns a view of the input tensor that reverses the order of the coefficients
01480 along a subset of the dimensions.  The argument reverse is an array of boolean
01481 values that indicates whether or not the order of the coefficients should be
01482 reversed along each of the dimensions.  This operation preserves the dimensions
01483 of the input tensor.
01484 
01485 For example this is what happens when you ```reverse()``` the first dimension
01486 of a 2D tensor:
01487 
01488     Eigen::Tensor<int, 2> a(4, 3);
01489     a.setValues(\{\{0, 100, 200\}, \{300, 400, 500\},
01490                 \{600, 700, 800\}, \{900, 1000, 1100\}\});
01491     Eigen::array<bool, 2> reverse(\{true, false\});
01492     Eigen::Tensor<int, 2> b = a.reverse(reverse);
01493     cout << "a" << endl << a << endl << "b" << endl << b << endl;
01494     =>
01495     a
01496        0   100   200
01497      300   400   500
01498      600   700   800
01499      900  1000  1100
01500     b
01501      900  1000  1100
01502      600   700   800
01503      300   400   500
01504        0   100   200
01505 
01506 
01507 ### <Operation> broadcast(const Broadcast& broadcast)
01508 
01509 Returns a view of the input tensor in which the input is replicated one to many
01510 times.
01511 The broadcast argument specifies how many copies of the input tensor need to be
01512 made in each of the dimensions.
01513 
01514     Eigen::Tensor<int, 2> a(2, 3);
01515     a.setValues(\{\{0, 100, 200\}, \{300, 400, 500\}\});
01516     Eigen::array<int, 2> bcast(\{3, 2\});
01517     Eigen::Tensor<int, 2> b = a.broadcast(bcast);
01518     cout << "a" << endl << a << endl << "b" << endl << b << endl;
01519     =>
01520     a
01521        0   100   200
01522      300   400   500
01523     b
01524        0   100   200    0   100   200
01525      300   400   500  300   400   500
01526        0   100   200    0   100   200
01527      300   400   500  300   400   500
01528        0   100   200    0   100   200
01529      300   400   500  300   400   500
01530 
01531 ### <Operation> concatenate(const OtherDerived& other, Axis axis)
01532 
01533 TODO
01534 
01535 ### <Operation>  pad(const PaddingDimensions& padding)
01536 
01537 Returns a view of the input tensor in which the input is padded with zeros.
01538 
01539     Eigen::Tensor<int, 2> a(2, 3);
01540     a.setValues(\{\{0, 100, 200\}, \{300, 400, 500\}\});
01541     Eigen::array<pair<int, int>, 2> paddings;
01542     paddings[0] = make\_pair(0, 1);
01543     paddings[1] = make\_pair(2, 3);
01544     Eigen::Tensor<int, 2> b = a.pad(paddings);
01545     cout << "a" << endl << a << endl << "b" << endl << b << endl;
01546     =>
01547     a
01548        0   100   200
01549      300   400   500
01550     b
01551        0     0     0    0
01552        0     0     0    0
01553        0   100   200    0
01554      300   400   500    0
01555        0     0     0    0
01556        0     0     0    0
01557        0     0     0    0
01558 
01559 
01560 ### <Operation>  extract\_patches(const PatchDims& patch\_dims)
01561 
01562 Returns a tensor of coefficient patches extracted from the input tensor, where
01563 each patch is of dimension specified by 'patch\_dims'. The returned tensor has
01564 one greater dimension than the input tensor, which is used to index each patch.
01565 The patch index in the output tensor depends on the data layout of the input
01566 tensor: the patch index is the last dimension ColMajor layout, and the first
01567 dimension in RowMajor layout.
01568 
01569 For example, given the following input tensor:
01570 
01571   Eigen::Tensor<float, 2, DataLayout> tensor(3,4);
01572   tensor.setValues(\{\{0.0f, 1.0f, 2.0f, 3.0f\},
01573                     \{4.0f, 5.0f, 6.0f, 7.0f\},
01574                     \{8.0f, 9.0f, 10.0f, 11.0f\}\});
01575 
01576   cout << "tensor: " << endl << tensor << endl;
01577 =>
01578 tensor:
01579  0   1   2   3
01580  4   5   6   7
01581  8   9  10  11
01582 
01583 Six 2x2 patches can be extracted and indexed using the following code:
01584 
01585   Eigen::Tensor<float, 3, DataLayout> patch;
01586   Eigen::array<ptrdiff\_t, 2> patch\_dims;
01587   patch\_dims[0] = 2;
01588   patch\_dims[1] = 2;
01589   patch = tensor.extract\_patches(patch\_dims);
01590   for (int k = 0; k < 6; ++k) \{
01591     cout << "patch index: " << k << endl;
01592     for (int i = 0; i < 2; ++i) \{
01593       for (int j = 0; j < 2; ++j) \{
01594         if (DataLayout == ColMajor) \{
01595           cout << patch(i, j, k) << " ";
01596         \} else \{
01597           cout << patch(k, i, j) << " ";
01598         \}
01599       \}
01600       cout << endl;
01601     \}
01602   \}
01603 
01604 This code results in the following output when the data layout is ColMajor:
01605 
01606 patch index: 0
01607 0 1
01608 4 5
01609 patch index: 1
01610 4 5
01611 8 9
01612 patch index: 2
01613 1 2
01614 5 6
01615 patch index: 3
01616 5 6
01617 9 10
01618 patch index: 4
01619 2 3
01620 6 7
01621 patch index: 5
01622 6 7
01623 10 11
01624 
01625 This code results in the following output when the data layout is RowMajor:
01626 (NOTE: the set of patches is the same as in ColMajor, but are indexed differently).
01627 
01628 patch index: 0
01629 0 1
01630 4 5
01631 patch index: 1
01632 1 2
01633 5 6
01634 patch index: 2
01635 2 3
01636 6 7
01637 patch index: 3
01638 4 5
01639 8 9
01640 patch index: 4
01641 5 6
01642 9 10
01643 patch index: 5
01644 6 7
01645 10 11
01646 
01647 ### <Operation>  extract\_image\_patches(const Index patch\_rows, const Index patch\_cols,
01648                           const Index row\_stride, const Index col\_stride,
01649                           const PaddingType padding\_type)
01650 
01651 Returns a tensor of coefficient image patches extracted from the input tensor,
01652 which is expected to have dimensions ordered as follows (depending on the data
01653 layout of the input tensor, and the number of additional dimensions 'N'):
01654 
01655 *) ColMajor
01656 1st dimension: channels (of size d)
01657 2nd dimension: rows (of size r)
01658 3rd dimension: columns (of size c)
01659 4th-Nth dimension: time (for video) or batch (for bulk processing).
01660 
01661 *) RowMajor (reverse order of ColMajor)
01662 1st-Nth dimension: time (for video) or batch (for bulk processing).
01663 N+1'th dimension: columns (of size c)
01664 N+2'th dimension: rows (of size r)
01665 N+3'th dimension: channels (of size d)
01666 
01667 The returned tensor has one greater dimension than the input tensor, which is
01668 used to index each patch. The patch index in the output tensor depends on the
01669 data layout of the input tensor: the patch index is the 4'th dimension in
01670 ColMajor layout, and the 4'th from the last dimension in RowMajor layout.
01671 
01672 For example, given the following input tensor with the following dimension
01673 sizes:
01674  *) depth:   2
01675  *) rows:    3
01676  *) columns: 5
01677  *) batch:   7
01678 
01679   Tensor<float, 4> tensor(2,3,5,7);
01680   Tensor<float, 4, RowMajor> tensor\_row\_major = tensor.swap\_layout();
01681 
01682 2x2 image patches can be extracted and indexed using the following code:
01683 
01684 *) 2D patch: ColMajor (patch indexed by second-to-last dimension)
01685   Tensor<float, 5> twod\_patch;
01686   twod\_patch = tensor.extract\_image\_patches<2, 2>();
01687   // twod\_patch.dimension(0) == 2
01688   // twod\_patch.dimension(1) == 2
01689   // twod\_patch.dimension(2) == 2
01690   // twod\_patch.dimension(3) == 3*5
01691   // twod\_patch.dimension(4) == 7
01692 
01693 *) 2D patch: RowMajor (patch indexed by the second dimension)
01694   Tensor<float, 5, RowMajor> twod\_patch\_row\_major;
01695   twod\_patch\_row\_major = tensor\_row\_major.extract\_image\_patches<2, 2>();
01696   // twod\_patch\_row\_major.dimension(0) == 7
01697   // twod\_patch\_row\_major.dimension(1) == 3*5
01698   // twod\_patch\_row\_major.dimension(2) == 2
01699   // twod\_patch\_row\_major.dimension(3) == 2
01700   // twod\_patch\_row\_major.dimension(4) == 2
01701 
01702 ## Special Operations
01703 
01704 ### <Operation> cast<T>()
01705 
01706 Returns a tensor of type T with the same dimensions as the original tensor.
01707 The returned tensor contains the values of the original tensor converted to
01708 type T.
01709 
01710     Eigen::Tensor<float, 2> a(2, 3);
01711     Eigen::Tensor<int, 2> b = a.cast<int>();
01712 
01713 This can be useful for example if you need to do element-wise division of
01714 Tensors of integers.  This is not currently supported by the Tensor library
01715 but you can easily cast the tensors to floats to do the division:
01716 
01717     Eigen::Tensor<int, 2> a(2, 3);
01718     a.setValues(\{\{0, 1, 2\}, \{3, 4, 5\}\});
01719     Eigen::Tensor<int, 2> b =
01720         (a.cast<float>() / a.constant(2).cast<float>()).cast<int>();
01721     cout << "a" << endl << a << endl << endl;
01722     cout << "b" << endl << b << endl << endl;
01723     =>
01724     a
01725     0 1 2
01726     3 4 5
01727 
01728     b
01729     0 0 1
01730     1 2 2
01731 
01732 
01733 ### <Operation>     eval()
01734 
01735 TODO
01736 
01737 
01738 ## Representation of scalar values
01739 
01740 Scalar values are often represented by tensors of size 1 and rank 1. It would be
01741 more logical and user friendly to use tensors of rank 0 instead. For example
01742 Tensor<T, N>::maximum() currently returns a Tensor<T, 1>. Similarly, the inner
01743 product of 2 1d tensors (through contractions) returns a 1d tensor. In the
01744 future these operations might be updated to return 0d tensors instead.
01745 
01746 ## Limitations
01747 
01748 *   The number of tensor dimensions is currently limited to 250 when using a
01749     compiler that supports cxx11. It is limited to only 5 for older compilers.
01750 *   The IndexList class requires a cxx11 compliant compiler. You can use an
01751     array of indices instead if you don't have access to a modern compiler.
01752 *   On GPUs only floating point values are properly tested and optimized for.
01753 *   Complex and integer values are known to be broken on GPUs. If you try to use
01754     them you'll most likely end up triggering a static assertion failure such as
01755     EIGEN\_STATIC\_ASSERT(packetSize > 1, YOU\_MADE\_A\_PROGRAMMING\_MISTAKE)
01756 
01757 
\end{DoxyCode}
